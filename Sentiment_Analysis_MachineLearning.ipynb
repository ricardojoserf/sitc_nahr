{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el corpus de [Canéphore](https://github.com/ressources-tal/canephore) que contiene tweets en francés anotados de opiniones de usuarios sobre el concurso de Miss France. Previamente, hemos podido descargarnos 2000 tweets (el corpus tiene 10000 pero la API de Twitter nos lo limitaba), que hemos agrupado en un mismo archivo (results.csv) junto con su polaridad (0-negativa, 1-positiva, Nan-neutra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>'Roussillon elle a marque 20points #MissFrance'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>'Miss Bretagne est trop vilaine. #MissFrance'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2567</th>\n",
       "      <td>'C'est à quel moment le défilé en bas de survêtement/sweat-shirt qui fait des bouloches/pantoufles/coiffure en freestyle ? #MissFrance'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>'Moi je vote pour une seconde année de @LauryThilleman #missfrance2012 #TF1'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>'Tout sa pour laissé Jean-Pierre se préparer en coulisse. A j'te jure ! #TF1'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3395</th>\n",
       "      <td>'Réunion : j'étais sûr. Alsace : aussi. Côte d'Azur : beurk. Pays de Loire : noooon ! Provence : mais beurk ! #MissFrance'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>'Moi je suis pour Miss Languedoc ! #MissFrance'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>'Ah ouais Miss Gwada cette année AIE !!! elle fait mal #Beauté #MissFrance'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4811</th>\n",
       "      <td>'Trop heureuse pour Miss Alsace ! :) #MissFrance'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>'Bon il dise les 12 finalistes #missfrance'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>'putain y'a des cuisseaux dis donc les précédents jury il était noir ou quoi ??? #MissFrance'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3790</th>\n",
       "      <td>'Ptet que le choix des finalistes aurait été plus juste si DSK avait été dans le jury... #MissFrance'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>'Euh Miss Guyane bof quoi .. #MissFrance'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>'Sur les 12 ya au moins 3 trans! Je les dégueule #MissFrance'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5160</th>\n",
       "      <td>'Jsuis contente que Miss France 2012 soit miss Alsace. #missfrance'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>'Miss Normandie n'est pas très belle #MissFrance'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>'@StoriaGiovanna Je ne sais pas sije vais tenir jusqu'au bout... Sylvie Tellier me donne déjà la gerbe #MissFrance'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>'Miss Lorraine a des cheveux synthétiques. #MissFrance'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>'Miss Bretagne n'est pas prise tant mieux elle est pas belle #missfrance'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>'Bon ben je suis pour Miss Pays de Loire #MissFrance'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                      content  \\\n",
       "2536                                                                                          'Roussillon elle a marque 20points #MissFrance'   \n",
       "2055                                                                                            'Miss Bretagne est trop vilaine. #MissFrance'   \n",
       "2567  'C'est à quel moment le défilé en bas de survêtement/sweat-shirt qui fait des bouloches/pantoufles/coiffure en freestyle ? #MissFrance'   \n",
       "820                                                              'Moi je vote pour une seconde année de @LauryThilleman #missfrance2012 #TF1'   \n",
       "161                                                             'Tout sa pour laissé Jean-Pierre se préparer en coulisse. A j'te jure ! #TF1'   \n",
       "3395               'Réunion : j'étais sûr. Alsace : aussi. Côte d'Azur : beurk. Pays de Loire : noooon ! Provence : mais beurk ! #MissFrance'   \n",
       "2745                                                                                          'Moi je suis pour Miss Languedoc ! #MissFrance'   \n",
       "821                                                               'Ah ouais Miss Gwada cette année AIE !!! elle fait mal #Beauté #MissFrance'   \n",
       "4811                                                                                        'Trop heureuse pour Miss Alsace ! :) #MissFrance'   \n",
       "1971                                                                                              'Bon il dise les 12 finalistes #missfrance'   \n",
       "1978                                            'putain y'a des cuisseaux dis donc les précédents jury il était noir ou quoi ??? #MissFrance'   \n",
       "3790                                    'Ptet que le choix des finalistes aurait été plus juste si DSK avait été dans le jury... #MissFrance'   \n",
       "1055                                                                                                'Euh Miss Guyane bof quoi .. #MissFrance'   \n",
       "2307                                                                            'Sur les 12 ya au moins 3 trans! Je les dégueule #MissFrance'   \n",
       "5160                                                                      'Jsuis contente que Miss France 2012 soit miss Alsace. #missfrance'   \n",
       "109                                                                                         'Miss Normandie n'est pas très belle #MissFrance'   \n",
       "64                        '@StoriaGiovanna Je ne sais pas sije vais tenir jusqu'au bout... Sylvie Tellier me donne déjà la gerbe #MissFrance'   \n",
       "524                                                                                   'Miss Lorraine a des cheveux synthétiques. #MissFrance'   \n",
       "3386                                                                'Miss Bretagne n'est pas prise tant mieux elle est pas belle #missfrance'   \n",
       "2672                                                                                    'Bon ben je suis pour Miss Pays de Loire #MissFrance'   \n",
       "\n",
       "     polarity  \n",
       "2536      Nan  \n",
       "2055        0  \n",
       "2567      Nan  \n",
       "820       Nan  \n",
       "161       Nan  \n",
       "3395        0  \n",
       "2745        1  \n",
       "821         0  \n",
       "4811        1  \n",
       "1971      Nan  \n",
       "1978      Nan  \n",
       "3790      Nan  \n",
       "1055        0  \n",
       "2307        0  \n",
       "5160        1  \n",
       "109         0  \n",
       "64          0  \n",
       "524         0  \n",
       "3386        0  \n",
       "2672        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_frances = pd.read_csv('results_extended.csv', encoding='utf-8')\n",
    "corpus_frances.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5546, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_frances.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos otro corpus descartando los tweets con polaridad neutra (Nan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2443, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_frances_sinNan = corpus_frances.query('polarity != \"Nan\"')\n",
    "corpus_frances_sinNan.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing & Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenenemos de nltk las palabras vacías francesas. Obtenemos también una lista de caracteres que se utilizan como puntuación (no añadimos ninguno porque son los mismos que los ingleses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download french stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "french_stopwords = stopwords.words('french')\n",
    "french_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "non_words = list(punctuation)\n",
    "non_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el algoritmo de stemming SnowballStemmer, disponible en francés también."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x7f2a188c1ac8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer       \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# based on http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "stemmer = SnowballStemmer('french')\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    # tokenize\n",
    "    tokens =  word_tokenize(text)\n",
    "\n",
    "    # stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems\n",
    "\n",
    "stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar con tres modelos distintos: LinearSVC, k-NN u Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/urano/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tres polaridades (positiva-1, negativa-0, neutra-Nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos los valores de polaridad en números enteros (polarity_num)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "content         object\n",
       "polarity        object\n",
       "polarity_num     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_frances['polarity_num'] = 0\n",
    "corpus_frances.polarity_num[corpus_frances.polarity.isin(['1'])] = 1\n",
    "corpus_frances.polarity_num[corpus_frances.polarity.isin(['Nan'])] = 2\n",
    "corpus_frances.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El corpus posee más tweets con polaridad neutra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.559502\n",
       "1    0.222322\n",
       "0    0.218175\n",
       "Name: polarity_num, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_frances.polarity_num.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario descargarse el paquete nltk (si no lo hemos hecho ya una primera vez)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos en GridSearch para encontrar los parámetros óptimos de cada modelo (esto solo es necesario hacerlo una vez)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['au', 'aux...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': (0.5, 1.9), 'vect__min_df': (10, 20, 50), 'vect__max_features': (500, 1000), 'vect__ngram_range': ((1, 1), (1, 2)), 'cls__C': (0.2, 0.5, 0.7), 'cls__loss': ('hinge', 'squared_hinge'), 'cls__max_iter': (500, 1000)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = french_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', LinearSVC()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__loss': ('hinge', 'squared_hinge'),\n",
    "    'cls__max_iter': (500, 1000)\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_lsvc = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='accuracy')\n",
    "grid_search_lsvc.fit(corpus_frances.content, corpus_frances.polarity_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_lsvc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['au', 'aux...owski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'))]),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'vect__max_df': array([ 0.5,  0.6,  0.7,  0.8,  0.9,  1. ,  1.1,  1.2,  1.3,  1.4,  1.5,\n",
       "        1.6,  1.7,  1.8,  1.9,  2. ,  2.1,  2.2,  2.3,  2.4,  2.5,  2.6,\n",
       "        2.7,  2.8,  2.9]), 'vect__min_df': array([10, 20, 30, 40, 50, 60, 70, 80, 90]), 'vect__max_features': array([...6, 41, 46, 51, 56, 61, 66, 71, 76, 81,\n",
       "       86, 91, 96]), 'cls__weights': ('uniform', 'distance')},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = french_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', KNeighborsClassifier()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__n_neighbors': (20,50,100),\n",
    "    'cls__weights': ('uniform', 'distance')\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_knn = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='accuracy')\n",
    "grid_search_knn.fit(corpus_frances.content, corpus_frances.polarity_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = french_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', MultinomialNB()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__alpha': (0.2,0,5,1),\n",
    "    'cls__fit_prior': ('True', 'False')\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_mnb = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='accuracy')\n",
    "grid_search_mnb.fit(corpus_frances.content, corpus_frances.polarity_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls__alpha': 0.28000000000000003,\n",
       " 'cls__fit_prior': 'True',\n",
       " 'vect__max_df': 0.5,\n",
       " 'vect__max_features': 500,\n",
       " 'vect__min_df': 10,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_mnb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conocer la eficacia de cada modelo, utilizamos los parámetros óptimos que hemos encontrado (es necesario cambiarlos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearSVC(C=.2, loss='hinge',max_iter=1000,multi_class='ovr',\n",
    "              random_state=None,\n",
    "              penalty='l2',\n",
    "              tol=0.0001\n",
    ")\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = french_stopwords,\n",
    "    min_df = 10,\n",
    "    max_df = 1.9,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(corpus_frances.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62430430430430428"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(corpus_frances)],\n",
    "    y=corpus_frances.polarity_num,\n",
    "    scoring='accuracy',\n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=81)\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = french_stopwords,\n",
    "    min_df = 20,\n",
    "    max_df = 1.9999999999999996,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=1100\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(corpus_frances.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62430430430430428"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(corpus_frances)],\n",
    "    y=corpus_frances.polarity_num,\n",
    "    scoring='accuracy',\n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MultinomialNB(alpha=0.28, fit_prior=\"True\")\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = french_stopwords,\n",
    "    min_df = 10,\n",
    "    max_df = 0.5,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=500\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(corpus_frances.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62430430430430428"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(corpus_frances)],\n",
    "    y=corpus_frances.polarity_num,\n",
    "    scoring='accuracy',\n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dos polaridades (positiva-1, negativa-0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos los valores de polaridad en números enteros (polarity_num)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:4702: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2881: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "content         object\n",
       "polarity        object\n",
       "polarity_num     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_frances_sinNan['polarity_num'] = 0\n",
    "corpus_frances_sinNan.polarity_num[corpus_frances_sinNan.polarity.isin(['1'])] = 1\n",
    "corpus_frances.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.504707\n",
       "0    0.495293\n",
       "Name: polarity_num, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_frances_sinNan.polarity_num.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos en GridSearch para encontrar los parámetros óptimos de cada modelo (esto solo es necesario hacerlo una vez)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['au', 'aux...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': (0.5, 1.9), 'vect__min_df': (10, 20, 50), 'vect__max_features': (500, 1000), 'vect__ngram_range': ((1, 1), (1, 2)), 'cls__C': (0.2, 0.5, 0.7), 'cls__loss': ('hinge', 'squared_hinge'), 'cls__max_iter': (500, 1000)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = french_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', LinearSVC()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__loss': ('hinge', 'squared_hinge'),\n",
    "    'cls__max_iter': (500, 1000)\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_lsvc = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='roc_auc')\n",
    "grid_search_lsvc.fit(corpus_frances_sinNan.content, corpus_frances_sinNan.polarity_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_lsvc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['au', 'aux...owski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'))]),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'vect__max_df': array([ 0.5,  0.6,  0.7,  0.8,  0.9,  1. ,  1.1,  1.2,  1.3,  1.4,  1.5,\n",
       "        1.6,  1.7,  1.8,  1.9,  2. ,  2.1,  2.2,  2.3,  2.4,  2.5,  2.6,\n",
       "        2.7,  2.8,  2.9]), 'vect__min_df': array([10, 20, 30, 40, 50, 60, 70, 80, 90]), 'vect__max_features': array([...6, 41, 46, 51, 56, 61, 66, 71, 76, 81,\n",
       "       86, 91, 96]), 'cls__weights': ('uniform', 'distance')},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = french_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', KNeighborsClassifier()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__n_neighbors': (20,50,100),\n",
    "    'cls__weights': ('uniform', 'distance')\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_knn = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='roc_auc')\n",
    "grid_search_knn.fit(corpus_frances_sinNan.content, corpus_frances_sinNan.polarity_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = french_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', MultinomialNB()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__alpha': (0.2,0,5,1),\n",
    "    'cls__fit_prior': ('True', 'False')\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_mnb = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='roc_auc')\n",
    "grid_search_mnb.fit(corpus_frances_sinNan.content, corpus_frances_sinNan.polarity_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls__alpha': 0.28000000000000003,\n",
       " 'cls__fit_prior': 'True',\n",
       " 'vect__max_df': 0.5,\n",
       " 'vect__max_features': 500,\n",
       " 'vect__min_df': 10,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_mnb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conocer la eficacia de cada modelo, utilizamos los parámetros óptimos que hemos encontrado (es necesario cambiarlos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearSVC(C=.2, loss='hinge',max_iter=1000,multi_class='ovr',\n",
    "              random_state=None,\n",
    "              penalty='l2',\n",
    "              tol=0.0001\n",
    ")\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = french_stopwords,\n",
    "    min_df = 10,\n",
    "    max_df = 1.9,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(corpus_frances_sinNan.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62430430430430428"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(corpus_frances_sinNan)],\n",
    "    y=corpus_frances_sinNan.polarity_num,\n",
    "    scoring='roc_auc',\n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=81)\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = french_stopwords,\n",
    "    min_df = 20,\n",
    "    max_df = 1.9999999999999996,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=1100\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(corpus_frances_sinNan.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62430430430430428"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(corpus_frances_sinNan)],\n",
    "    y=corpus_frances_sinNan.polarity_num,\n",
    "    scoring='roc_auc',\n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MultinomialNB(alpha=0.28, fit_prior=\"True\")\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = french_stopwords,\n",
    "    min_df = 10,\n",
    "    max_df = 0.5,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=500\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(corpus_frances_sinNan.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62430430430430428"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(corpus_frances_sinNan)],\n",
    "    y=corpus_frances_sinNan.polarity_num,\n",
    "    scoring='roc_auc',\n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de polaridad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Utilizamos el modelo entrenado para el análisis de sentimientos en los tweets descargados **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos uno de los archivos csv con los tweets de una de las regiones de Francia (es necesario hacerlo con todos los csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>rts</th>\n",
       "      <th>place</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-06 16:02:55</td>\n",
       "      <td>RT @Freezze: \"Excusez moi mais ... Mais .. Pourrait on évoquer le ... S'il vous plait ? Est ce que ... Oh et puis démerdez vous tiens\"  #20…</td>\n",
       "      <td>Ju'</td>\n",
       "      <td>7435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-06 16:02:38</td>\n",
       "      <td>RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA</td>\n",
       "      <td>Marc barbier</td>\n",
       "      <td>10654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-06 16:02:38</td>\n",
       "      <td>RT @ErenJaeger95: Normalement #2017LeDébat aurait dû ce passé comme ça 😭😭😭😭 https://t.co/XSQEn936G7</td>\n",
       "      <td>Dany</td>\n",
       "      <td>1568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-06 16:01:43</td>\n",
       "      <td>RT @EmmanuelMacron: Je veux présider le pays. #2017LeDébat</td>\n",
       "      <td>APPELEZ MOI ZA2👸🏻</td>\n",
       "      <td>1763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-05-06 15:59:41</td>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>Clem's</td>\n",
       "      <td>15120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  \\\n",
       "0  2017-05-06 16:02:55   \n",
       "1  2017-05-06 16:02:38   \n",
       "2  2017-05-06 16:02:38   \n",
       "3  2017-05-06 16:01:43   \n",
       "4  2017-05-06 15:59:41   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0  RT @Freezze: \"Excusez moi mais ... Mais .. Pourrait on évoquer le ... S'il vous plait ? Est ce que ... Oh et puis démerdez vous tiens\"  #20…   \n",
       "1                                                                    RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA   \n",
       "2                                           RT @ErenJaeger95: Normalement #2017LeDébat aurait dû ce passé comme ça 😭😭😭😭 https://t.co/XSQEn936G7   \n",
       "3                                                                                    RT @EmmanuelMacron: Je veux présider le pays. #2017LeDébat   \n",
       "4                                                               RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "\n",
       "                user    rts place  lon  lat  \n",
       "0                Ju'   7435   NaN  NaN  NaN  \n",
       "1       Marc barbier  10654   NaN  NaN  NaN  \n",
       "2               Dany   1568   NaN  NaN  NaN  \n",
       "3  APPELEZ MOI ZA2👸🏻   1763   NaN  NaN  NaN  \n",
       "4             Clem's  15120   NaN  NaN  NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('Ile-de-France.csv', encoding='utf-8')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Detección del lenguaje **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos aseguramos que todos los tweets están escritos en francés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import langid\n",
    "from langdetect import detect\n",
    "import textblob\n",
    "\n",
    "def langid_safe(tweet):\n",
    "    try:\n",
    "        return langid.classify(tweet)[0]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "def langdetect_safe(tweet):\n",
    "    try:\n",
    "        return detect(tweet)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "def textblob_safe(tweet):\n",
    "    try:\n",
    "        return textblob.TextBlob(tweet).detect_language()\n",
    "    except Exception as e:\n",
    "        pass   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this will take a loong time.\n",
    "tweets['lang_langid'] = tweets.text.apply(langid_safe)\n",
    "tweets['lang_langdetect'] = tweets.text.apply(langdetect_safe)\n",
    "tweets['lang_textblob'] = tweets.text.apply(textblob_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>rts</th>\n",
       "      <th>place</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>lang_langid</th>\n",
       "      <th>lang_langdetect</th>\n",
       "      <th>lang_textblob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-06 16:02:55</td>\n",
       "      <td>RT @Freezze: \"Excusez moi mais ... Mais .. Pourrait on évoquer le ... S'il vous plait ? Est ce que ... Oh et puis démerdez vous tiens\"  #20…</td>\n",
       "      <td>Ju'</td>\n",
       "      <td>7435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-06 16:02:38</td>\n",
       "      <td>RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA</td>\n",
       "      <td>Marc barbier</td>\n",
       "      <td>10654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it</td>\n",
       "      <td>en</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-06 16:02:38</td>\n",
       "      <td>RT @ErenJaeger95: Normalement #2017LeDébat aurait dû ce passé comme ça 😭😭😭😭 https://t.co/XSQEn936G7</td>\n",
       "      <td>Dany</td>\n",
       "      <td>1568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-06 16:01:43</td>\n",
       "      <td>RT @EmmanuelMacron: Je veux présider le pays. #2017LeDébat</td>\n",
       "      <td>APPELEZ MOI ZA2👸🏻</td>\n",
       "      <td>1763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-05-06 15:59:41</td>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>Clem's</td>\n",
       "      <td>15120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-05-06 15:57:39</td>\n",
       "      <td>RT @TeamMacron2017: Louis Aliot, du FN, avec Camel Bechikh représentant de l'UOIF. Marine Le Pen devrait balayer devant sa porte #2017LeDeb…</td>\n",
       "      <td>Dominique Baiguini</td>\n",
       "      <td>2113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-05-06 15:57:35</td>\n",
       "      <td>RT @EmmanuelMacron: #2017LeDébat en 5 minutes ! https://t.co/xAeJKpjDKu</td>\n",
       "      <td>🐑</td>\n",
       "      <td>3171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-05-06 15:57:31</td>\n",
       "      <td>RT @EmmanuelMacron: Madame Le Pen, la France mérite mieux que vous. #2017LeDébat</td>\n",
       "      <td>twenty2</td>\n",
       "      <td>16683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-05-06 15:57:05</td>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>Emilie Arwidson</td>\n",
       "      <td>15120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-05-06 15:56:58</td>\n",
       "      <td>RT @Freezze: C'est bon elle a vrillé complet #2017LeDebat https://t.co/ldRd72wX7d</td>\n",
       "      <td>princesse_loulou13</td>\n",
       "      <td>12121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-05-06 15:56:27</td>\n",
       "      <td>RT @ErenJaeger95: Normalement #2017LeDébat aurait dû ce passé comme ça 😭😭😭😭 https://t.co/XSQEn936G7</td>\n",
       "      <td>lénine</td>\n",
       "      <td>1568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017-05-06 15:55:49</td>\n",
       "      <td>RT @Charli_thr: quand ca commence à parler de l'islam #2017LeDébat https://t.co/VG7dcnWTDK</td>\n",
       "      <td>🐉</td>\n",
       "      <td>1792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-05-06 15:55:18</td>\n",
       "      <td>RT @franceinfoplus: #2017LeDébat résumé en 13 dessins de presse drôles et moqueurs  https://t.co/lW59cawr87 https://t.co/MCKre3aNJQ</td>\n",
       "      <td>Christine Morlet CSP</td>\n",
       "      <td>367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-05-06 15:55:10</td>\n",
       "      <td>RT @laink: J'ai loupé un super débat apparemment :( #2017LeDebat</td>\n",
       "      <td>Skye Lancaster</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-05-06 15:54:54</td>\n",
       "      <td>RT @ZohraBitan: Par la puissance de la médiocrité et méchanceté de Marine Le Pen, Macron n'a pas eu de mal à briller ... #2017LeDebat</td>\n",
       "      <td>Nicole Maria FQSP©</td>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-05-06 15:54:31</td>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>sarachou🌹</td>\n",
       "      <td>15120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017-05-06 15:54:15</td>\n",
       "      <td>RT @mkfrison: Ça marche avec toutes les chansons !!! #2017LeDébat https://t.co/wVNpaVwjxu</td>\n",
       "      <td>léa</td>\n",
       "      <td>2099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017-05-06 15:54:13</td>\n",
       "      <td>RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA</td>\n",
       "      <td>💁🏻</td>\n",
       "      <td>10654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017-05-06 15:53:32</td>\n",
       "      <td>RT @Marine2017_EN: 🔴KEY PHRASE \"Mr #Macron, in any case, #France will be governed by a #Woman: either ME or Mrs. #MERKEL !\" #2017LeDebat @M…</td>\n",
       "      <td>Hiep Pham</td>\n",
       "      <td>511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017-05-06 15:52:32</td>\n",
       "      <td>RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA</td>\n",
       "      <td>Levi-sama in my ♡</td>\n",
       "      <td>10654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it</td>\n",
       "      <td>nl</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017-05-06 15:51:42</td>\n",
       "      <td>RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA</td>\n",
       "      <td>FreeYoni⚡️</td>\n",
       "      <td>10654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it</td>\n",
       "      <td>en</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017-05-06 15:51:28</td>\n",
       "      <td>RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA</td>\n",
       "      <td>云霞</td>\n",
       "      <td>10654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it</td>\n",
       "      <td>en</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017-05-06 15:51:22</td>\n",
       "      <td>RT @topito_com: Le temps de parole de Marine :\\n- 10 % son programme\\n- 90 % \"Macron il est méchant\" #2017LeDebat</td>\n",
       "      <td>Salomé ❤</td>\n",
       "      <td>2456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017-05-06 15:51:05</td>\n",
       "      <td>RT @CyrusNorth: La prochaine fois, prenez pas des journalistes ! Ramenez directement Super Nanny et Pascal le Grand Frère ! \\n#2017LeDebat</td>\n",
       "      <td>Famas_972</td>\n",
       "      <td>1549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2017-05-06 15:50:35</td>\n",
       "      <td>RT @Freezze: C'est bon elle a vrillé complet #2017LeDebat https://t.co/ldRd72wX7d</td>\n",
       "      <td>FreeYoni⚡️</td>\n",
       "      <td>12121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2017-05-06 15:48:49</td>\n",
       "      <td>RT @EmmanuelMacron: Madame Le Pen, la France mérite mieux que vous. #2017LeDébat</td>\n",
       "      <td>mims</td>\n",
       "      <td>16683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017-05-06 15:47:51</td>\n",
       "      <td>RT @Sylvqin: T'as pas besoin de parler de ton programme si t'insultes l'autre candidat pendant 3h #2017LeDébat https://t.co/ocJ33xJMTd</td>\n",
       "      <td>Eva Plsn</td>\n",
       "      <td>32986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017-05-06 15:47:24</td>\n",
       "      <td>RT @Freezze: C'est bon elle a vrillé complet #2017LeDebat https://t.co/ldRd72wX7d</td>\n",
       "      <td>🇫🇷🇪🇺vote Macron</td>\n",
       "      <td>12121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017-05-06 15:45:57</td>\n",
       "      <td>RT @topito_com: Quand le prochain métro arrive dans 17 minutes #Nathalie2017 #2017LeDebat https://t.co/DgOIXbleIz</td>\n",
       "      <td>Melehan</td>\n",
       "      <td>693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017-05-06 15:45:41</td>\n",
       "      <td>RT @Marine2017_EN: 🔴KEY PHRASE \"Mr #Macron, in any case, #France will be governed by a #Woman: either ME or Mrs. #MERKEL !\" #2017LeDebat @M…</td>\n",
       "      <td>Robin Delmar Ph.D</td>\n",
       "      <td>511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>2017-05-06 09:42:53</td>\n",
       "      <td>RT @lounatssr: Elle veut retirer le mariage pour tous mais là ça défend les homosexuels?????? c'est trop #2017LeDébat</td>\n",
       "      <td>El</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>2017-05-06 09:42:40</td>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>Lou'🔥</td>\n",
       "      <td>15120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>2017-05-06 09:42:40</td>\n",
       "      <td>RT @Sylvqin: T'as pas besoin de parler de ton programme si t'insultes l'autre candidat pendant 3h #2017LeDébat https://t.co/ocJ33xJMTd</td>\n",
       "      <td>Eki Lotest</td>\n",
       "      <td>32986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>2017-05-06 09:42:34</td>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>God</td>\n",
       "      <td>15120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>2017-05-06 09:42:05</td>\n",
       "      <td>RT @CarlMeeus: C'est le moment parfait il me semble pour reprendre ce propos de Raymond Aron #2017LeDebat https://t.co/9V0V0lpPnB</td>\n",
       "      <td>L'Echiquier social</td>\n",
       "      <td>341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>2017-05-06 09:41:50</td>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>élise tramini</td>\n",
       "      <td>15120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>2017-05-06 09:41:28</td>\n",
       "      <td>RT @Sylvqin: TOP MARINE A DIT ISLAM RECORD BATTU #2017LeDébat https://t.co/Dcx7OCOrz6</td>\n",
       "      <td>Arsenio</td>\n",
       "      <td>322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hu</td>\n",
       "      <td>en</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>2017-05-06 09:41:16</td>\n",
       "      <td>RT @dupontaignan: M.Macron, petit télégraphiste de Mme Merkel, veut appliquer la feuille  de route de Bruxelles avec l'argent des Français.…</td>\n",
       "      <td>LISA HADDAD</td>\n",
       "      <td>1723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>2017-05-06 09:40:40</td>\n",
       "      <td>RT @Alextrxm: \"Madame Le Pen, vous pouvez nous parler de votre programme ? De vos propositions ?\" #2017LeDebat https://t.co/9z89ri4Br1</td>\n",
       "      <td>My Name Is Kappa</td>\n",
       "      <td>1678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>2017-05-06 09:40:35</td>\n",
       "      <td>RT @LePoint: \"Le naufrage Marine Le Pen.\" L'édito de @SCoignard https://t.co/zRVSMhPCtS #2017LeDebat #Debat2017 https://t.co/o6xZeWetOv</td>\n",
       "      <td>DRUGUET GERARD</td>\n",
       "      <td>58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>2017-05-06 09:40:34</td>\n",
       "      <td>RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA</td>\n",
       "      <td>Nico</td>\n",
       "      <td>10654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it</td>\n",
       "      <td>en</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>2017-05-06 09:40:21</td>\n",
       "      <td>RT @exdrn: - t'a pas besoin d'expliquer ton programme si t'en a pas #GrandDebat2017 #2017LeDébat https://t.co/6qR7SKBZLm</td>\n",
       "      <td>dragoon79</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>2017-05-06 09:39:45</td>\n",
       "      <td>RT @BuzzFeedFRpol: Marine Le Pen en mode actor's studio #2017LeDebat https://t.co/eQUD9SBmED</td>\n",
       "      <td>Axaurus</td>\n",
       "      <td>245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>nl</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>2017-05-06 09:39:43</td>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>️</td>\n",
       "      <td>15120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>2017-05-06 09:39:16</td>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>aly</td>\n",
       "      <td>15120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>2017-05-06 09:38:56</td>\n",
       "      <td>RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA</td>\n",
       "      <td>Suzon</td>\n",
       "      <td>10654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it</td>\n",
       "      <td>en</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>2017-05-06 09:38:50</td>\n",
       "      <td>RT @Sarah_Menr: Le Pen présidente ? \\n\"Une chaise, une table, un banc sera élu plutôt qu'elle dans ce pays.\" #RendezNousMelenchon #LeGrandDe…</td>\n",
       "      <td>Gunel</td>\n",
       "      <td>441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>2017-05-06 09:38:46</td>\n",
       "      <td>RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA</td>\n",
       "      <td>aly</td>\n",
       "      <td>10654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it</td>\n",
       "      <td>nl</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>2017-05-06 09:38:14</td>\n",
       "      <td>RT @NormanDesVideos: J'ai l'impression que vous vous énervez, non c'est vous qui vous énervez, non c'est vous qui vous énervez, non c'est v…</td>\n",
       "      <td>MatMat</td>\n",
       "      <td>3051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>2017-05-06 09:38:01</td>\n",
       "      <td>RT @Marine2017_EN: Mr Macron ! Judicial laxity causes an explosion of insecurity, in our cities as well as in the countryside #2017LeDebat…</td>\n",
       "      <td>carl hall</td>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>2017-05-06 09:38:00</td>\n",
       "      <td>RT @Marine2017_EN: #Macron forbids CEOs to defend themselves against #islamist claims #2017LeDebat @MLP_officiel https://t.co/mBQEgDPl2z</td>\n",
       "      <td>carl hall</td>\n",
       "      <td>158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>2017-05-06 09:37:57</td>\n",
       "      <td>RT @Marine2017_EN: It has been 25 years since you promised this \"social Europe\" and we only got unemployment, deindustrialization, relocati…</td>\n",
       "      <td>carl hall</td>\n",
       "      <td>357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>2017-05-06 09:37:56</td>\n",
       "      <td>RT @christineboutin: #2017ledebat #Macron  n'a rien compris sur la monnaie commune et le franc ! Tout est faux! C'est affligeant de le voir…</td>\n",
       "      <td>GUERMOND</td>\n",
       "      <td>324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>2017-05-06 09:37:52</td>\n",
       "      <td>RT @NicolasKanaan: Je suis sûr que Macron a parié 10000€ avec un pote qu'il devait placer \"Poudre de perlinpinpin\" dans une phrase\\n\\n#LeGran…</td>\n",
       "      <td>MatMat</td>\n",
       "      <td>1763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>2017-05-06 09:37:51</td>\n",
       "      <td>RT @Marine2017_EN: 🔴KEY PHRASE \"Mr #Macron, in any case, #France will be governed by a #Woman: either ME or Mrs. #MERKEL !\" #2017LeDebat @M…</td>\n",
       "      <td>carl hall</td>\n",
       "      <td>511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2017-05-06 09:37:44</td>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>clxvis 🌹</td>\n",
       "      <td>15120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2017-05-06 09:37:31</td>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>PARIS SAINT-GERMAIN❤</td>\n",
       "      <td>15120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2017-05-06 09:37:11</td>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>Arsenio</td>\n",
       "      <td>15120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2017-05-06 09:36:56</td>\n",
       "      <td>RT @ActusStar: Demain ? #jevotepour #2017LeDebat #MacronLeaks \\n\\n@MLP_officiel / @EmmanuelMacron \\n\\n#RT EN MASSE POUR + DE VISIBILITÉ</td>\n",
       "      <td>Team MMLP</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fr</td>\n",
       "      <td>en</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2017-05-06 09:36:55</td>\n",
       "      <td>RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA</td>\n",
       "      <td>houaria🇩🇿</td>\n",
       "      <td>10654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it</td>\n",
       "      <td>nl</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    time  \\\n",
       "0    2017-05-06 16:02:55   \n",
       "1    2017-05-06 16:02:38   \n",
       "2    2017-05-06 16:02:38   \n",
       "3    2017-05-06 16:01:43   \n",
       "4    2017-05-06 15:59:41   \n",
       "5    2017-05-06 15:57:39   \n",
       "6    2017-05-06 15:57:35   \n",
       "7    2017-05-06 15:57:31   \n",
       "8    2017-05-06 15:57:05   \n",
       "9    2017-05-06 15:56:58   \n",
       "10   2017-05-06 15:56:27   \n",
       "11   2017-05-06 15:55:49   \n",
       "12   2017-05-06 15:55:18   \n",
       "13   2017-05-06 15:55:10   \n",
       "14   2017-05-06 15:54:54   \n",
       "15   2017-05-06 15:54:31   \n",
       "16   2017-05-06 15:54:15   \n",
       "17   2017-05-06 15:54:13   \n",
       "18   2017-05-06 15:53:32   \n",
       "19   2017-05-06 15:52:32   \n",
       "20   2017-05-06 15:51:42   \n",
       "21   2017-05-06 15:51:28   \n",
       "22   2017-05-06 15:51:22   \n",
       "23   2017-05-06 15:51:05   \n",
       "24   2017-05-06 15:50:35   \n",
       "25   2017-05-06 15:48:49   \n",
       "26   2017-05-06 15:47:51   \n",
       "27   2017-05-06 15:47:24   \n",
       "28   2017-05-06 15:45:57   \n",
       "29   2017-05-06 15:45:41   \n",
       "..                   ...   \n",
       "970  2017-05-06 09:42:53   \n",
       "971  2017-05-06 09:42:40   \n",
       "972  2017-05-06 09:42:40   \n",
       "973  2017-05-06 09:42:34   \n",
       "974  2017-05-06 09:42:05   \n",
       "975  2017-05-06 09:41:50   \n",
       "976  2017-05-06 09:41:28   \n",
       "977  2017-05-06 09:41:16   \n",
       "978  2017-05-06 09:40:40   \n",
       "979  2017-05-06 09:40:35   \n",
       "980  2017-05-06 09:40:34   \n",
       "981  2017-05-06 09:40:21   \n",
       "982  2017-05-06 09:39:45   \n",
       "983  2017-05-06 09:39:43   \n",
       "984  2017-05-06 09:39:16   \n",
       "985  2017-05-06 09:38:56   \n",
       "986  2017-05-06 09:38:50   \n",
       "987  2017-05-06 09:38:46   \n",
       "988  2017-05-06 09:38:14   \n",
       "989  2017-05-06 09:38:01   \n",
       "990  2017-05-06 09:38:00   \n",
       "991  2017-05-06 09:37:57   \n",
       "992  2017-05-06 09:37:56   \n",
       "993  2017-05-06 09:37:52   \n",
       "994  2017-05-06 09:37:51   \n",
       "995  2017-05-06 09:37:44   \n",
       "996  2017-05-06 09:37:31   \n",
       "997  2017-05-06 09:37:11   \n",
       "998  2017-05-06 09:36:56   \n",
       "999  2017-05-06 09:36:55   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "0      RT @Freezze: \"Excusez moi mais ... Mais .. Pourrait on évoquer le ... S'il vous plait ? Est ce que ... Oh et puis démerdez vous tiens\"  #20…   \n",
       "1                                                                        RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA   \n",
       "2                                               RT @ErenJaeger95: Normalement #2017LeDébat aurait dû ce passé comme ça 😭😭😭😭 https://t.co/XSQEn936G7   \n",
       "3                                                                                        RT @EmmanuelMacron: Je veux présider le pays. #2017LeDébat   \n",
       "4                                                                   RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "5      RT @TeamMacron2017: Louis Aliot, du FN, avec Camel Bechikh représentant de l'UOIF. Marine Le Pen devrait balayer devant sa porte #2017LeDeb…   \n",
       "6                                                                           RT @EmmanuelMacron: #2017LeDébat en 5 minutes ! https://t.co/xAeJKpjDKu   \n",
       "7                                                                  RT @EmmanuelMacron: Madame Le Pen, la France mérite mieux que vous. #2017LeDébat   \n",
       "8                                                                   RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "9                                                                 RT @Freezze: C'est bon elle a vrillé complet #2017LeDebat https://t.co/ldRd72wX7d   \n",
       "10                                              RT @ErenJaeger95: Normalement #2017LeDébat aurait dû ce passé comme ça 😭😭😭😭 https://t.co/XSQEn936G7   \n",
       "11                                                       RT @Charli_thr: quand ca commence à parler de l'islam #2017LeDébat https://t.co/VG7dcnWTDK   \n",
       "12              RT @franceinfoplus: #2017LeDébat résumé en 13 dessins de presse drôles et moqueurs  https://t.co/lW59cawr87 https://t.co/MCKre3aNJQ   \n",
       "13                                                                                 RT @laink: J'ai loupé un super débat apparemment :( #2017LeDebat   \n",
       "14            RT @ZohraBitan: Par la puissance de la médiocrité et méchanceté de Marine Le Pen, Macron n'a pas eu de mal à briller ... #2017LeDebat   \n",
       "15                                                                  RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "16                                                        RT @mkfrison: Ça marche avec toutes les chansons !!! #2017LeDébat https://t.co/wVNpaVwjxu   \n",
       "17                                                                       RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA   \n",
       "18     RT @Marine2017_EN: 🔴KEY PHRASE \"Mr #Macron, in any case, #France will be governed by a #Woman: either ME or Mrs. #MERKEL !\" #2017LeDebat @M…   \n",
       "19                                                                       RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA   \n",
       "20                                                                       RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA   \n",
       "21                                                                       RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA   \n",
       "22                                RT @topito_com: Le temps de parole de Marine :\\n- 10 % son programme\\n- 90 % \"Macron il est méchant\" #2017LeDebat   \n",
       "23       RT @CyrusNorth: La prochaine fois, prenez pas des journalistes ! Ramenez directement Super Nanny et Pascal le Grand Frère ! \\n#2017LeDebat   \n",
       "24                                                                RT @Freezze: C'est bon elle a vrillé complet #2017LeDebat https://t.co/ldRd72wX7d   \n",
       "25                                                                 RT @EmmanuelMacron: Madame Le Pen, la France mérite mieux que vous. #2017LeDébat   \n",
       "26           RT @Sylvqin: T'as pas besoin de parler de ton programme si t'insultes l'autre candidat pendant 3h #2017LeDébat https://t.co/ocJ33xJMTd   \n",
       "27                                                                RT @Freezze: C'est bon elle a vrillé complet #2017LeDebat https://t.co/ldRd72wX7d   \n",
       "28                                RT @topito_com: Quand le prochain métro arrive dans 17 minutes #Nathalie2017 #2017LeDebat https://t.co/DgOIXbleIz   \n",
       "29     RT @Marine2017_EN: 🔴KEY PHRASE \"Mr #Macron, in any case, #France will be governed by a #Woman: either ME or Mrs. #MERKEL !\" #2017LeDebat @M…   \n",
       "..                                                                                                                                              ...   \n",
       "970                           RT @lounatssr: Elle veut retirer le mariage pour tous mais là ça défend les homosexuels?????? c'est trop #2017LeDébat   \n",
       "971                                                                 RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "972          RT @Sylvqin: T'as pas besoin de parler de ton programme si t'insultes l'autre candidat pendant 3h #2017LeDébat https://t.co/ocJ33xJMTd   \n",
       "973                                                                 RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "974               RT @CarlMeeus: C'est le moment parfait il me semble pour reprendre ce propos de Raymond Aron #2017LeDebat https://t.co/9V0V0lpPnB   \n",
       "975                                                                 RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "976                                                           RT @Sylvqin: TOP MARINE A DIT ISLAM RECORD BATTU #2017LeDébat https://t.co/Dcx7OCOrz6   \n",
       "977    RT @dupontaignan: M.Macron, petit télégraphiste de Mme Merkel, veut appliquer la feuille  de route de Bruxelles avec l'argent des Français.…   \n",
       "978          RT @Alextrxm: \"Madame Le Pen, vous pouvez nous parler de votre programme ? De vos propositions ?\" #2017LeDebat https://t.co/9z89ri4Br1   \n",
       "979         RT @LePoint: \"Le naufrage Marine Le Pen.\" L'édito de @SCoignard https://t.co/zRVSMhPCtS #2017LeDebat #Debat2017 https://t.co/o6xZeWetOv   \n",
       "980                                                                      RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA   \n",
       "981                        RT @exdrn: - t'a pas besoin d'expliquer ton programme si t'en a pas #GrandDebat2017 #2017LeDébat https://t.co/6qR7SKBZLm   \n",
       "982                                                    RT @BuzzFeedFRpol: Marine Le Pen en mode actor's studio #2017LeDebat https://t.co/eQUD9SBmED   \n",
       "983                                                                 RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "984                                                                 RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "985                                                                      RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA   \n",
       "986   RT @Sarah_Menr: Le Pen présidente ? \\n\"Une chaise, une table, un banc sera élu plutôt qu'elle dans ce pays.\" #RendezNousMelenchon #LeGrandDe…   \n",
       "987                                                                      RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA   \n",
       "988    RT @NormanDesVideos: J'ai l'impression que vous vous énervez, non c'est vous qui vous énervez, non c'est vous qui vous énervez, non c'est v…   \n",
       "989     RT @Marine2017_EN: Mr Macron ! Judicial laxity causes an explosion of insecurity, in our cities as well as in the countryside #2017LeDebat…   \n",
       "990        RT @Marine2017_EN: #Macron forbids CEOs to defend themselves against #islamist claims #2017LeDebat @MLP_officiel https://t.co/mBQEgDPl2z   \n",
       "991    RT @Marine2017_EN: It has been 25 years since you promised this \"social Europe\" and we only got unemployment, deindustrialization, relocati…   \n",
       "992    RT @christineboutin: #2017ledebat #Macron  n'a rien compris sur la monnaie commune et le franc ! Tout est faux! C'est affligeant de le voir…   \n",
       "993  RT @NicolasKanaan: Je suis sûr que Macron a parié 10000€ avec un pote qu'il devait placer \"Poudre de perlinpinpin\" dans une phrase\\n\\n#LeGran…   \n",
       "994    RT @Marine2017_EN: 🔴KEY PHRASE \"Mr #Macron, in any case, #France will be governed by a #Woman: either ME or Mrs. #MERKEL !\" #2017LeDebat @M…   \n",
       "995                                                                 RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "996                                                                 RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "997                                                                 RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "998         RT @ActusStar: Demain ? #jevotepour #2017LeDebat #MacronLeaks \\n\\n@MLP_officiel / @EmmanuelMacron \\n\\n#RT EN MASSE POUR + DE VISIBILITÉ   \n",
       "999                                                                      RT @Freezze: RT si t'as rien compris. #2017LeDebat https://t.co/8mT06MGiZA   \n",
       "\n",
       "                     user    rts place  lon  lat lang_langid lang_langdetect  \\\n",
       "0                     Ju'   7435   NaN  NaN  NaN          fr              fr   \n",
       "1            Marc barbier  10654   NaN  NaN  NaN          it              en   \n",
       "2                    Dany   1568   NaN  NaN  NaN          fr              fr   \n",
       "3       APPELEZ MOI ZA2👸🏻   1763   NaN  NaN  NaN          fr              fr   \n",
       "4                  Clem's  15120   NaN  NaN  NaN          fr              fr   \n",
       "5      Dominique Baiguini   2113   NaN  NaN  NaN          fr              fr   \n",
       "6                       🐑   3171   NaN  NaN  NaN          fr              fr   \n",
       "7                 twenty2  16683   NaN  NaN  NaN          fr              fr   \n",
       "8         Emilie Arwidson  15120   NaN  NaN  NaN          fr              fr   \n",
       "9      princesse_loulou13  12121   NaN  NaN  NaN          fr              fr   \n",
       "10                 lénine   1568   NaN  NaN  NaN          fr              fr   \n",
       "11                      🐉   1792   NaN  NaN  NaN          fr              fr   \n",
       "12   Christine Morlet CSP    367   NaN  NaN  NaN          fr              fr   \n",
       "13         Skye Lancaster     54   NaN  NaN  NaN          fr              fr   \n",
       "14     Nicole Maria FQSP©     56   NaN  NaN  NaN          fr              fr   \n",
       "15              sarachou🌹  15120   NaN  NaN  NaN          fr              fr   \n",
       "16                    léa   2099   NaN  NaN  NaN          fr              fr   \n",
       "17                     💁🏻  10654   NaN  NaN  NaN          it              it   \n",
       "18              Hiep Pham    511   NaN  NaN  NaN          en              en   \n",
       "19      Levi-sama in my ♡  10654   NaN  NaN  NaN          it              nl   \n",
       "20             FreeYoni⚡️  10654   NaN  NaN  NaN          it              en   \n",
       "21                     云霞  10654   NaN  NaN  NaN          it              en   \n",
       "22               Salomé ❤   2456   NaN  NaN  NaN          fr              fr   \n",
       "23              Famas_972   1549   NaN  NaN  NaN          fr              fr   \n",
       "24             FreeYoni⚡️  12121   NaN  NaN  NaN          fr              fr   \n",
       "25                   mims  16683   NaN  NaN  NaN          fr              fr   \n",
       "26               Eva Plsn  32986   NaN  NaN  NaN          fr              fr   \n",
       "27        🇫🇷🇪🇺vote Macron  12121   NaN  NaN  NaN          fr              fr   \n",
       "28                Melehan    693   NaN  NaN  NaN          fr              fr   \n",
       "29      Robin Delmar Ph.D    511   NaN  NaN  NaN          en              en   \n",
       "..                    ...    ...   ...  ...  ...         ...             ...   \n",
       "970                    El      7   NaN  NaN  NaN          fr              fr   \n",
       "971                 Lou'🔥  15120   NaN  NaN  NaN          fr              fr   \n",
       "972            Eki Lotest  32986   NaN  NaN  NaN          fr              fr   \n",
       "973                   God  15120   NaN  NaN  NaN          fr              fr   \n",
       "974    L'Echiquier social    341   NaN  NaN  NaN          fr              fr   \n",
       "975         élise tramini  15120   NaN  NaN  NaN          fr              fr   \n",
       "976               Arsenio    322   NaN  NaN  NaN          hu              en   \n",
       "977           LISA HADDAD   1723   NaN  NaN  NaN          fr              fr   \n",
       "978      My Name Is Kappa   1678   NaN  NaN  NaN          fr              fr   \n",
       "979        DRUGUET GERARD     58   NaN  NaN  NaN          fr              fr   \n",
       "980                  Nico  10654   NaN  NaN  NaN          it              en   \n",
       "981             dragoon79    151   NaN  NaN  NaN          fr              fr   \n",
       "982               Axaurus    245   NaN  NaN  NaN          fr              nl   \n",
       "983                     ️  15120   NaN  NaN  NaN          fr              fr   \n",
       "984                   aly  15120   NaN  NaN  NaN          fr              fr   \n",
       "985                 Suzon  10654   NaN  NaN  NaN          it              en   \n",
       "986                 Gunel    441   NaN  NaN  NaN          fr              fr   \n",
       "987                   aly  10654   NaN  NaN  NaN          it              nl   \n",
       "988                MatMat   3051   NaN  NaN  NaN          fr              fr   \n",
       "989             carl hall     95   NaN  NaN  NaN          en              en   \n",
       "990             carl hall    158   NaN  NaN  NaN          en              en   \n",
       "991             carl hall    357   NaN  NaN  NaN          en              en   \n",
       "992              GUERMOND    324   NaN  NaN  NaN          fr              fr   \n",
       "993                MatMat   1763   NaN  NaN  NaN          fr              fr   \n",
       "994             carl hall    511   NaN  NaN  NaN          en              en   \n",
       "995              clxvis 🌹  15120   NaN  NaN  NaN          fr              fr   \n",
       "996  PARIS SAINT-GERMAIN❤  15120   NaN  NaN  NaN          fr              fr   \n",
       "997               Arsenio  15120   NaN  NaN  NaN          fr              fr   \n",
       "998             Team MMLP     86   NaN  NaN  NaN          fr              en   \n",
       "999             houaria🇩🇿  10654   NaN  NaN  NaN          it              nl   \n",
       "\n",
       "    lang_textblob  \n",
       "0              fr  \n",
       "1              fr  \n",
       "2              fr  \n",
       "3              fr  \n",
       "4              fr  \n",
       "5              fr  \n",
       "6              fr  \n",
       "7              fr  \n",
       "8              fr  \n",
       "9              fr  \n",
       "10             fr  \n",
       "11             fr  \n",
       "12             fr  \n",
       "13             fr  \n",
       "14             fr  \n",
       "15             fr  \n",
       "16             fr  \n",
       "17             fr  \n",
       "18             en  \n",
       "19             fr  \n",
       "20             fr  \n",
       "21             fr  \n",
       "22             fr  \n",
       "23             fr  \n",
       "24             fr  \n",
       "25             fr  \n",
       "26             fr  \n",
       "27             fr  \n",
       "28             fr  \n",
       "29             en  \n",
       "..            ...  \n",
       "970            fr  \n",
       "971            fr  \n",
       "972            fr  \n",
       "973            fr  \n",
       "974            fr  \n",
       "975            fr  \n",
       "976            fr  \n",
       "977            fr  \n",
       "978            fr  \n",
       "979            fr  \n",
       "980            fr  \n",
       "981            fr  \n",
       "982            en  \n",
       "983            fr  \n",
       "984            fr  \n",
       "985            fr  \n",
       "986            fr  \n",
       "987            fr  \n",
       "988            fr  \n",
       "989            en  \n",
       "990            en  \n",
       "991            en  \n",
       "992            fr  \n",
       "993            fr  \n",
       "994            en  \n",
       "995            fr  \n",
       "996            fr  \n",
       "997            fr  \n",
       "998            fr  \n",
       "999            fr  \n",
       "\n",
       "[1000 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(956, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets.query(''' lang_langdetect == 'fr' or lang_langid == 'fr' or lang_textblob == 'fr'  ''')\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Predicción con los parámetros óptimos y el modelo entrenado **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario meter los parámetros óptimos que hemos encontrado en el apartado anterior, tanto para los de tres polaridades como los binarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = french_stopwords,\n",
    "            min_df = 10,\n",
    "            max_df = 0.5,\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=500\n",
    "            )),\n",
    "    ('cls', LinearSVC(C=.2, loss='hinge',max_iter=1000,multi_class='ovr',\n",
    "             random_state=None,\n",
    "             penalty='l2',\n",
    "             tol=0.0001\n",
    "             )),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(corpus_frances.content, corpus_frances.polarity_num)\n",
    "tweets['lsvc'] = pipeline.predict(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = french_stopwords,\n",
    "            min_df = 10,\n",
    "            max_df = 0.5,\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=500\n",
    "            )),\n",
    "    ('cls', KNeighborsClassifier(n_neighbors=81)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(corpus_frances.content, corpus_frances.polarity_num)\n",
    "tweets['knn'] = pipeline.predict(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = french_stopwords,\n",
    "            min_df = 10,\n",
    "            max_df = 0.5,\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=500\n",
    "            )),\n",
    "    ('cls', MultinomialNB(alpha=0.28, fit_prior=\"True\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(corpus_frances.content, corpus_frances.polarity_num)\n",
    "tweets['mnb'] = pipeline.predict(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = french_stopwords,\n",
    "            min_df = 10,\n",
    "            max_df = 0.5,\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=500\n",
    "            )),\n",
    "    ('cls', LinearSVC(C=.2, loss='hinge',max_iter=1000,multi_class='ovr',\n",
    "             random_state=None,\n",
    "             penalty='l2',\n",
    "             tol=0.0001\n",
    "             )),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(corpus_frances_sinNan.content, corpus_frances_sinNan.polarity_num)\n",
    "tweets['lsvc_bin'] = pipeline.predict(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = french_stopwords,\n",
    "            min_df = 10,\n",
    "            max_df = 0.5,\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=500\n",
    "            )),\n",
    "    ('cls', KNeighborsClassifier(n_neighbors=81)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(corpus_frances_sinNan.content, corpus_frances_sinNan.polarity_num)\n",
    "tweets['knn_bin'] = pipeline.predict(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = french_stopwords,\n",
    "            min_df = 10,\n",
    "            max_df = 0.5,\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=500\n",
    "            )),\n",
    "    ('cls', MultinomialNB(alpha=0.28, fit_prior=\"True\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(corpus_frances_sinNan.content, corpus_frances_sinNan.polarity_num)\n",
    "tweets['mnb_bin'] = pipeline.predict(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>RT @fligoupier: Tu me manques, petit ange parti trop tôt... 😢 #2017LeDebat https://t.co/UTUFL693MB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>RT @Sylvqin: T'as pas besoin de parler de ton programme si t'insultes l'autre candidat pendant 3h #2017LeDébat https://t.co/ocJ33xJMTd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>RT @EmmanuelMacron: Madame Le Pen, la France mérite mieux que vous. #2017LeDébat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>RT @mkfrison: Ça marche avec toutes les chansons !!! #2017LeDébat https://t.co/wVNpaVwjxu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>RT @gmaujean: Ce soir, les fact-checkers en burn-out avec MLP #2017LeDébat https://t.co/amSUCwwPtl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>RT @Neacko83: En 2002, Chirac disait : \"on ne débat pas avec l'extreme droite\".\\n15 ans après, en voyant #LePen, on comprend mieux pourquoi.…</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>RT @TheClownOfParis: \"non mais a un moment donné ... LA FEMME A BOOBA !\" #2017LeDebat https://t.co/8K8lYgs4SQ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>RT @NasNacera: Le FN promeut le « Made in France » mais fabrique ses tee-shirts en Asie. Patriote tu dis ?  #2017LeDebat https://t.co/SBd28…</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>nos 15 tweets préférés sur #2017LeDebat histoire de rire/pleurer un coup. https://t.co/j2ia6vlQNK… https://t.co/XFqH60vi7H</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @EmmanuelMacron: #2017LeDébat en 5 minutes ! https://t.co/xAeJKpjDKu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>RT @EMacron_2017: [#JeVoteMacron 🇫🇷 #2017LeDébat] Carte blanche d'E. #Macron sur l'#OutreMer et pour les personnes en situation de #handica…</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>RT @Freezze: C'est bon elle a vrillé complet #2017LeDebat https://t.co/ldRd72wX7d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>RT @ErenJaeger95: Normalement #2017LeDébat aurait dû ce passé comme ça 😭😭😭😭 https://t.co/XSQEn936G7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>RT @Sylvqin: T'as pas besoin de parler de ton programme si t'insultes l'autre candidat pendant 3h #2017LeDébat https://t.co/ocJ33xJMTd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>RT @2017Soutiens: Voici le véritable parti de la haine. Et il s'appelle En Marche ! #BayrouGate #Cathedrale #Reims #JeVotePour #2017LeDebat…</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              text  \\\n",
       "341                                             RT @fligoupier: Tu me manques, petit ange parti trop tôt... 😢 #2017LeDebat https://t.co/UTUFL693MB   \n",
       "907         RT @Sylvqin: T'as pas besoin de parler de ton programme si t'insultes l'autre candidat pendant 3h #2017LeDébat https://t.co/ocJ33xJMTd   \n",
       "882                                                                RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "595                                                               RT @EmmanuelMacron: Madame Le Pen, la France mérite mieux que vous. #2017LeDébat   \n",
       "860                                                      RT @mkfrison: Ça marche avec toutes les chansons !!! #2017LeDébat https://t.co/wVNpaVwjxu   \n",
       "568                                                                RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "872                                             RT @gmaujean: Ce soir, les fact-checkers en burn-out avec MLP #2017LeDébat https://t.co/amSUCwwPtl   \n",
       "616  RT @Neacko83: En 2002, Chirac disait : \"on ne débat pas avec l'extreme droite\".\\n15 ans après, en voyant #LePen, on comprend mieux pourquoi.…   \n",
       "458                                  RT @TheClownOfParis: \"non mais a un moment donné ... LA FEMME A BOOBA !\" #2017LeDebat https://t.co/8K8lYgs4SQ   \n",
       "491   RT @NasNacera: Le FN promeut le « Made in France » mais fabrique ses tee-shirts en Asie. Patriote tu dis ?  #2017LeDebat https://t.co/SBd28…   \n",
       "88                      nos 15 tweets préférés sur #2017LeDebat histoire de rire/pleurer un coup. https://t.co/j2ia6vlQNK… https://t.co/XFqH60vi7H   \n",
       "6                                                                          RT @EmmanuelMacron: #2017LeDébat en 5 minutes ! https://t.co/xAeJKpjDKu   \n",
       "808                                                                RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "362   RT @EMacron_2017: [#JeVoteMacron 🇫🇷 #2017LeDébat] Carte blanche d'E. #Macron sur l'#OutreMer et pour les personnes en situation de #handica…   \n",
       "214                                                              RT @Freezze: C'est bon elle a vrillé complet #2017LeDebat https://t.co/ldRd72wX7d   \n",
       "546                                                                RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "728                                            RT @ErenJaeger95: Normalement #2017LeDébat aurait dû ce passé comme ça 😭😭😭😭 https://t.co/XSQEn936G7   \n",
       "35                                                                 RT @deleteitugly: Meilleur moment du débat #2017LeDebat https://t.co/8N9Dmgl2mH   \n",
       "147         RT @Sylvqin: T'as pas besoin de parler de ton programme si t'insultes l'autre candidat pendant 3h #2017LeDébat https://t.co/ocJ33xJMTd   \n",
       "160   RT @2017Soutiens: Voici le véritable parti de la haine. Et il s'appelle En Marche ! #BayrouGate #Cathedrale #Reims #JeVotePour #2017LeDebat…   \n",
       "\n",
       "     polarity  \n",
       "341         0  \n",
       "907         0  \n",
       "882         0  \n",
       "595         0  \n",
       "860         0  \n",
       "568         0  \n",
       "872         1  \n",
       "616         0  \n",
       "458         0  \n",
       "491         0  \n",
       "88          1  \n",
       "6           0  \n",
       "808         0  \n",
       "362         0  \n",
       "214         1  \n",
       "546         0  \n",
       "728         1  \n",
       "35          0  \n",
       "147         0  \n",
       "160         0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[['text', 'lsvc', 'knn', 'mnb', 'lsvc_bin', 'knn_bin', 'mnb_bin']].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos los tweets con su polaridad y coordenadas para situarlos en el mapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets[['text', 'lat', 'lon', 'lsvc', 'knn', 'mnb', 'lsvc_bin', 'knn_bin', 'mnb_bin']].to_csv('Ile-de-France_polarity_latlon.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
