{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el corpus de [Canéphore](https://github.com/ressources-tal/canephore) que contiene tweets en francés anotados de opiniones de usuarios sobre el concurso de Miss France. Previamente, hemos podido descargarnos 2000 tweets (el corpus tiene 10000 pero la API de Twitter nos lo limitaba), que hemos agrupado en un mismo archivo (results.csv) junto con su polaridad (0-negativa, 1-positiva, Nan-neutra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3554</th>\n",
       "      <td>'Eh ben notre miss côte d'azur est soit dauphine soit Miss France bravo elle est bien !!!Dommage que Languedoc ne soit pas passée#MissFrance'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>'Sylvie Tellier a grossi #ça me console suis pas la seule!!!! #MissFrance'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>'Miss Réunion n'est pas magnifique mais je la trouve mignonne elle a quelque chose en plus. #MissFrance'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3519</th>\n",
       "      <td>'Bon Miss Réunion !!!! #MissFrance'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>'Mes favorites: Pays de loire et Roussillon #MissFrance'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4824</th>\n",
       "      <td>'Alsace P1!!! #MissFrance'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>'Je paris sur Miss Réunion. #MissFrance'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152</th>\n",
       "      <td>'HS à l'hôtel après cette grosse journée parisienne mais le cœur est à #Brest ! &amp;lt3 #MissFrance #Bretagne #monpays'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4668</th>\n",
       "      <td>'@Brit_CiciAddict Dommage mec !! =) Miss Alsace a gagné !!! #MissFrance'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>'Et à part les Miss France il se passe quoi ce soir dans le monde ? #missfrance @BenThev @zappette @Daphne_Burki01 @FlorencePorcel @Vinvin'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5475</th>\n",
       "      <td>'JT TF1 : Zoom sur Nancy la perle de la Lorraine - Le journal du Week-end - Replay</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>'Ho putain. Lorie faut se calmer sur les décoltés là. #tf1'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>'Miss Alsace vient de ruiner toutes ses chances. #MissFrance'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>'Perso j'aime pas leurs robes :s #Missfrance'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5357</th>\n",
       "      <td>'Bon ben voila l'Alsace vous avez Delphine Wespiser Miss France 2012 :) #missfrance'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4339</th>\n",
       "      <td>'@Maud_en_Vogue Miss Tahiti était trop petite :( C'est dommage elle méritait sa place en finale #missfrance'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>'Allez Miss Champagne ! #MissFrance'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>'Bon c'est simple dès qu'une miss parle d'équitation elle sort sa m'insuporte! miss cote d'azur #missfrance'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3191</th>\n",
       "      <td>'#MissFrance Pleeease dites moi si Miss Alsace est encore en lice! :-D'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4333</th>\n",
       "      <td>'Ouh la vache ! Laurie Machin = actor studio ! É-mo-tion ! #missfrance'</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            content  \\\n",
       "3554  'Eh ben notre miss côte d'azur est soit dauphine soit Miss France bravo elle est bien !!!Dommage que Languedoc ne soit pas passée#MissFrance'   \n",
       "279                                                                      'Sylvie Tellier a grossi #ça me console suis pas la seule!!!! #MissFrance'   \n",
       "3749                                       'Miss Réunion n'est pas magnifique mais je la trouve mignonne elle a quelque chose en plus. #MissFrance'   \n",
       "3519                                                                                                            'Bon Miss Réunion !!!! #MissFrance'   \n",
       "2699                                                                                       'Mes favorites: Pays de loire et Roussillon #MissFrance'   \n",
       "4824                                                                                                                     'Alsace P1!!! #MissFrance'   \n",
       "1712                                                                                                       'Je paris sur Miss Réunion. #MissFrance'   \n",
       "3152                           'HS à l'hôtel après cette grosse journée parisienne mais le cœur est à #Brest ! &lt3 #MissFrance #Bretagne #monpays'   \n",
       "4668                                                                       '@Brit_CiciAddict Dommage mec !! =) Miss Alsace a gagné !!! #MissFrance'   \n",
       "2273    'Et à part les Miss France il se passe quoi ce soir dans le monde ? #missfrance @BenThev @zappette @Daphne_Burki01 @FlorencePorcel @Vinvin'   \n",
       "5475                                                            'JT TF1 : Zoom sur Nancy la perle de la Lorraine - Le journal du Week-end - Replay    \n",
       "3218                                                                                    'Ho putain. Lorie faut se calmer sur les décoltés là. #tf1'   \n",
       "2732                                                                                  'Miss Alsace vient de ruiner toutes ses chances. #MissFrance'   \n",
       "844                                                                                                   'Perso j'aime pas leurs robes :s #Missfrance'   \n",
       "5357                                                           'Bon ben voila l'Alsace vous avez Delphine Wespiser Miss France 2012 :) #missfrance'   \n",
       "4339                                   '@Maud_en_Vogue Miss Tahiti était trop petite :( C'est dommage elle méritait sa place en finale #missfrance'   \n",
       "728                                                                                                            'Allez Miss Champagne ! #MissFrance'   \n",
       "613                                    'Bon c'est simple dès qu'une miss parle d'équitation elle sort sa m'insuporte! miss cote d'azur #missfrance'   \n",
       "3191                                                                        '#MissFrance Pleeease dites moi si Miss Alsace est encore en lice! :-D'   \n",
       "4333                                                                        'Ouh la vache ! Laurie Machin = actor studio ! É-mo-tion ! #missfrance'   \n",
       "\n",
       "     polarity  \n",
       "3554        1  \n",
       "279         0  \n",
       "3749        0  \n",
       "3519        1  \n",
       "2699        1  \n",
       "4824      Nan  \n",
       "1712      Nan  \n",
       "3152      Nan  \n",
       "4668      Nan  \n",
       "2273      Nan  \n",
       "5475      Nan  \n",
       "3218      Nan  \n",
       "2732      Nan  \n",
       "844         0  \n",
       "5357      Nan  \n",
       "4339        1  \n",
       "728         1  \n",
       "613         0  \n",
       "3191      Nan  \n",
       "4333      Nan  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_frances = pd.read_csv('results_extended.csv', encoding='utf-8')\n",
    "corpus_frances.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5546, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_frances.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos otro corpus descartando los tweets con polaridad neutra (Nan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2443, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_frances_sinNan = corpus_frances.query('polarity != \"Nan\"')\n",
    "corpus_frances_sinNan.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing & Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenenemos de nltk las palabras vacías francesas. Obtenemos también una lista de caracteres que se utilizan como puntuación (no añadimos ninguno porque son los mismos que los ingleses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download french stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "french_stopwords = stopwords.words('french')\n",
    "french_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "non_words = list(punctuation)\n",
    "non_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el algoritmo de stemming SnowballStemmer, disponible en francés también."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x7fe576b27cf8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer       \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# based on http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "stemmer = SnowballStemmer('french')\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    # tokenize\n",
    "    tokens =  word_tokenize(text)\n",
    "\n",
    "    # stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems\n",
    "\n",
    "stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar con tres modelos distintos: LinearSVC, k-NN u Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tres polaridades (positiva-1, negativa-0, neutra-Nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos los valores de polaridad en números enteros (polarity_num)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "content         object\n",
       "polarity        object\n",
       "polarity_num     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_frances['polarity_num'] = 0\n",
    "corpus_frances.polarity_num[corpus_frances.polarity.isin(['1'])] = 1\n",
    "corpus_frances.polarity_num[corpus_frances.polarity.isin(['Nan'])] = 2\n",
    "corpus_frances.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El corpus posee más tweets con polaridad neutra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.559502\n",
       "1    0.222322\n",
       "0    0.218175\n",
       "Name: polarity_num, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_frances.polarity_num.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario descargarse el paquete nltk (si no lo hemos hecho ya una primera vez)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos en GridSearch para encontrar los parámetros óptimos de cada modelo (esto solo es necesario hacerlo una vez)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['au', 'aux...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': (0.5, 1.9), 'vect__min_df': (10, 20, 50), 'vect__max_features': (500, 1000), 'vect__ngram_range': ((1, 1), (1, 2)), 'cls__C': (0.2, 0.5, 0.7), 'cls__loss': ('hinge', 'squared_hinge'), 'cls__max_iter': (500, 1000)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = french_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', LinearSVC()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__loss': ('hinge', 'squared_hinge'),\n",
    "    'cls__max_iter': (500, 1000)\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_lsvc = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='accuracy')\n",
    "grid_search_lsvc.fit(corpus_frances.content, corpus_frances.polarity_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_lsvc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['au', 'aux...owski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'))]),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'vect__max_df': array([ 0.5,  0.6,  0.7,  0.8,  0.9,  1. ,  1.1,  1.2,  1.3,  1.4,  1.5,\n",
       "        1.6,  1.7,  1.8,  1.9,  2. ,  2.1,  2.2,  2.3,  2.4,  2.5,  2.6,\n",
       "        2.7,  2.8,  2.9]), 'vect__min_df': array([10, 20, 30, 40, 50, 60, 70, 80, 90]), 'vect__max_features': array([...6, 41, 46, 51, 56, 61, 66, 71, 76, 81,\n",
       "       86, 91, 96]), 'cls__weights': ('uniform', 'distance')},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = french_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', KNeighborsClassifier()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__n_neighbors': (20,50,100),\n",
    "    'cls__weights': ('uniform', 'distance')\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_knn = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='accuracy')\n",
    "grid_search_knn.fit(corpus_frances.content, corpus_frances.polarity_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = french_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', MultinomialNB()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__alpha': (0.2,0,5,1),\n",
    "    'cls__fit_prior': ('True', 'False')\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_mnb = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='accuracy')\n",
    "grid_search_mnb.fit(corpus_frances.content, corpus_frances.polarity_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls__alpha': 0.28000000000000003,\n",
       " 'cls__fit_prior': 'True',\n",
       " 'vect__max_df': 0.5,\n",
       " 'vect__max_features': 500,\n",
       " 'vect__min_df': 10,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_mnb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conocer la eficacia de cada modelo, utilizamos los parámetros óptimos que hemos encontrado (es necesario cambiarlos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(C=.5, loss='hinge',max_iter=500,multi_class='ovr',\n",
    "              random_state=None,\n",
    "              penalty='l2',\n",
    "              tol=0.0001\n",
    ")\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = french_stopwords,\n",
    "    min_df = 10,\n",
    "    max_df = 0.5,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(corpus_frances.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7035675025205711"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(corpus_frances)],\n",
    "    y=corpus_frances.polarity_num,\n",
    "    scoring='accuracy',\n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = french_stopwords,\n",
    "    min_df = 20,\n",
    "    max_df = 0.5,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=500\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(corpus_frances.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59557615377109963"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(corpus_frances)],\n",
    "    y=corpus_frances.polarity_num,\n",
    "    scoring='accuracy',\n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB(alpha=1, fit_prior=\"True\")\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = french_stopwords,\n",
    "    min_df = 10,\n",
    "    max_df = 0.5,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=500\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(corpus_frances.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6694975119523856"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(corpus_frances)],\n",
    "    y=corpus_frances.polarity_num,\n",
    "    scoring='accuracy',\n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dos polaridades (positiva-1, negativa-0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos los valores de polaridad en números enteros (polarity_num)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:4702: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2881: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "content         object\n",
       "polarity        object\n",
       "polarity_num     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_frances_sinNan['polarity_num'] = 0\n",
    "corpus_frances_sinNan.polarity_num[corpus_frances_sinNan.polarity.isin(['1'])] = 1\n",
    "corpus_frances.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.504707\n",
       "0    0.495293\n",
       "Name: polarity_num, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_frances_sinNan.polarity_num.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos en GridSearch para encontrar los parámetros óptimos de cada modelo (esto solo es necesario hacerlo una vez)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['au', 'aux...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_df': (0.5, 1.9), 'vect__min_df': (10, 20, 50), 'vect__max_features': (500, 1000), 'vect__ngram_range': ((1, 1), (1, 2)), 'cls__C': (0.2, 0.5, 0.7), 'cls__loss': ('hinge', 'squared_hinge'), 'cls__max_iter': (500, 1000)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = french_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', LinearSVC()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__loss': ('hinge', 'squared_hinge'),\n",
    "    'cls__max_iter': (500, 1000)\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_lsvc = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='roc_auc')\n",
    "grid_search_lsvc.fit(corpus_frances_sinNan.content, corpus_frances_sinNan.polarity_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_lsvc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['au', 'aux...owski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'))]),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'vect__max_df': array([ 0.5,  0.6,  0.7,  0.8,  0.9,  1. ,  1.1,  1.2,  1.3,  1.4,  1.5,\n",
       "        1.6,  1.7,  1.8,  1.9,  2. ,  2.1,  2.2,  2.3,  2.4,  2.5,  2.6,\n",
       "        2.7,  2.8,  2.9]), 'vect__min_df': array([10, 20, 30, 40, 50, 60, 70, 80, 90]), 'vect__max_features': array([...6, 41, 46, 51, 56, 61, 66, 71, 76, 81,\n",
       "       86, 91, 96]), 'cls__weights': ('uniform', 'distance')},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = french_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', KNeighborsClassifier()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__n_neighbors': (20,50,100),\n",
    "    'cls__weights': ('uniform', 'distance')\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_knn = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='roc_auc')\n",
    "grid_search_knn.fit(corpus_frances_sinNan.content, corpus_frances_sinNan.polarity_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = french_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', MultinomialNB()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'cls__alpha': (0.2,0,5,1),\n",
    "    'cls__fit_prior': ('True', 'False')\n",
    "}\n",
    "\n",
    "\n",
    "grid_search_mnb = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='roc_auc')\n",
    "grid_search_mnb.fit(corpus_frances_sinNan.content, corpus_frances_sinNan.polarity_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls__alpha': 0.28000000000000003,\n",
       " 'cls__fit_prior': 'True',\n",
       " 'vect__max_df': 0.5,\n",
       " 'vect__max_features': 500,\n",
       " 'vect__min_df': 10,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_mnb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conocer la eficacia de cada modelo, utilizamos los parámetros óptimos que hemos encontrado (es necesario cambiarlos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearSVC(C=.2, loss='squared_hinge',max_iter=500,multi_class='ovr',\n",
    "              random_state=None,\n",
    "              penalty='l2',\n",
    "              tol=0.0001\n",
    ")\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = french_stopwords,\n",
    "    min_df = 10,\n",
    "    max_df = 0.5,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=500\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(corpus_frances_sinNan.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86399954734649564"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(corpus_frances_sinNan)],\n",
    "    y=corpus_frances_sinNan.polarity_num,\n",
    "    scoring='roc_auc',\n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=50)\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = french_stopwords,\n",
    "    min_df = 10,\n",
    "    max_df = 0.5,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=500\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(corpus_frances_sinNan.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79113436355529954"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(corpus_frances_sinNan)],\n",
    "    y=corpus_frances_sinNan.polarity_num,\n",
    "    scoring='roc_auc',\n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MultinomialNB(alpha=0.2, fit_prior=\"True\")\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = french_stopwords,\n",
    "    min_df = 10,\n",
    "    max_df = 0.5,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=500\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(corpus_frances_sinNan.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85011709417124293"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(corpus_frances_sinNan)],\n",
    "    y=corpus_frances_sinNan.polarity_num,\n",
    "    scoring='roc_auc',\n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de polaridad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Utilizamos el modelo entrenado para el análisis de sentimientos en los tweets descargados **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos uno de los archivos csv con los tweets de una de las regiones de Francia (es necesario hacerlo con todos los csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>rts</th>\n",
       "      <th>place</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>dic</th>\n",
       "      <th>dic_rounded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-07 08:42:40</td>\n",
       "      <td>RT @TabNacim: Allez on remballe tout fin du débat\\n#2017LeDebat</td>\n",
       "      <td>nome</td>\n",
       "      <td>155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47751</td>\n",
       "      <td>1675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-07 06:35:50</td>\n",
       "      <td>Nan mais ce soir je crois on va jouer nos vies ptn🤞🏼🍣 #2017LeDebat</td>\n",
       "      <td>Paul MONMARTEAU</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47751</td>\n",
       "      <td>1675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-05-06 23:29:23</td>\n",
       "      <td>#2017LeDebat il s'agit de choisir le moins con</td>\n",
       "      <td>nαhwel</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47751</td>\n",
       "      <td>1675</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-06 20:18:14</td>\n",
       "      <td>RT @ddlarry13: #2017LeDebat j'espère un grand écart entre les deux candidats demain comme ça la haine ne reviendra plus</td>\n",
       "      <td>Good Kisser</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47751</td>\n",
       "      <td>1675</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-05-06 19:54:52</td>\n",
       "      <td>#2017LeDebat j'espère un grand écart entre les deux candidats demain comme ça la haine ne reviendra plus</td>\n",
       "      <td>nαhwel</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47751</td>\n",
       "      <td>1675</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   time  \\\n",
       "0  2017-05-07 08:42:40    \n",
       "1  2017-05-07 06:35:50    \n",
       "2  2017-05-06 23:29:23    \n",
       "3  2017-05-06 20:18:14    \n",
       "4  2017-05-06 19:54:52    \n",
       "\n",
       "                                                                                                                       text  \\\n",
       "0                                                         RT @TabNacim: Allez on remballe tout fin du débat\\n#2017LeDebat     \n",
       "1                                                       Nan mais ce soir je crois on va jouer nos vies ptn🤞🏼🍣 #2017LeDebat    \n",
       "2                                                                           #2017LeDebat il s'agit de choisir le moins con    \n",
       "3  RT @ddlarry13: #2017LeDebat j'espère un grand écart entre les deux candidats demain comme ça la haine ne reviendra plus    \n",
       "4                 #2017LeDebat j'espère un grand écart entre les deux candidats demain comme ça la haine ne reviendra plus    \n",
       "\n",
       "               user  rts place    lon   lat  dic dic_rounded  \n",
       "0              nome  155  NaN   47751  1675  0.0         Nan  \n",
       "1  Paul MONMARTEAU     0  NaN   47751  1675  0.0         Nan  \n",
       "2           nαhwel     0  NaN   47751  1675 -1.0           0  \n",
       "3      Good Kisser     1  NaN   47751  1675  0.5           1  \n",
       "4           nαhwel     1  NaN   47751  1675  0.5           1  "
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('tweets_debat_dic/dic_Centre-Val de Loire.csv', encoding='utf-8')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(954, 9)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Predicción con los parámetros óptimos y el modelo entrenado **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario meter los parámetros óptimos que hemos encontrado en el apartado anterior, tanto para los de tres polaridades como los binarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = french_stopwords,\n",
    "            min_df = 10,\n",
    "            max_df = 0.5,\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=1000\n",
    "            )),\n",
    "    ('cls', LinearSVC(C=.5, loss='hinge',max_iter=500,multi_class='ovr',\n",
    "             random_state=None,\n",
    "             penalty='l2',\n",
    "             tol=0.0001\n",
    "             )),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(corpus_frances.content, corpus_frances.polarity_num)\n",
    "tweets['lsvc'] = pipeline.predict(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = french_stopwords,\n",
    "            min_df = 20,\n",
    "            max_df = 0.5,\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=500\n",
    "            )),\n",
    "    ('cls', KNeighborsClassifier(n_neighbors=20)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(corpus_frances.content, corpus_frances.polarity_num)\n",
    "tweets['knn'] = pipeline.predict(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = french_stopwords,\n",
    "            min_df = 10,\n",
    "            max_df = 0.5,\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=500\n",
    "            )),\n",
    "    ('cls', MultinomialNB(alpha=1, fit_prior=\"True\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(corpus_frances.content, corpus_frances.polarity_num)\n",
    "tweets['mnb'] = pipeline.predict(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = french_stopwords,\n",
    "            min_df = 10,\n",
    "            max_df = 0.5,\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=500\n",
    "            )),\n",
    "    ('cls', LinearSVC(C=.2, loss='squared_hinge',max_iter=500,multi_class='ovr',\n",
    "             random_state=None,\n",
    "             penalty='l2',\n",
    "             tol=0.0001\n",
    "             )),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(corpus_frances_sinNan.content, corpus_frances_sinNan.polarity_num)\n",
    "tweets['lsvc_bin'] = pipeline.predict(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = french_stopwords,\n",
    "            min_df = 10,\n",
    "            max_df = 0.5,\n",
    "            ngram_range=(1, 1),\n",
    "            max_features=500\n",
    "            )),\n",
    "    ('cls', KNeighborsClassifier(n_neighbors=50)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(corpus_frances_sinNan.content, corpus_frances_sinNan.polarity_num)\n",
    "tweets['knn_bin'] = pipeline.predict(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = french_stopwords,\n",
    "            min_df = 10,\n",
    "            max_df = 0.5,\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=500\n",
    "            )),\n",
    "    ('cls', MultinomialNB(alpha=0.2, fit_prior=\"True\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline.fit(corpus_frances_sinNan.content, corpus_frances_sinNan.polarity_num)\n",
    "tweets['mnb_bin'] = pipeline.predict(tweets.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>dic_rounded</th>\n",
       "      <th>lsvc</th>\n",
       "      <th>knn</th>\n",
       "      <th>mnb</th>\n",
       "      <th>lsvc_bin</th>\n",
       "      <th>knn_bin</th>\n",
       "      <th>mnb_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>#2017LeDebat il l'a laisse pas parler c'est abusé que il coupe la parole mdrr</td>\n",
       "      <td>Nan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RT @romainribas: #2017LeDebat @EmmanuelMacron \"80% de nos médicaments sont importés\" or avec un TVA FN à 23% les prix vont augmenter (+ inf…</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>RT @romainribas: #2017LeDebat @EmmanuelMacron \"@MLP_officiel profite de l'échec et de la colère Elle utilise toute sa conclusion pr insulte…</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>RT @Alex_Quenet: Elle a complètement pété un câble 😂😂 #2017LeDebat</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>Mon dieu la cassos 😂😂😂😂😂 #Debat2017 #2017LeDebat</td>\n",
       "      <td>Nan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>#2017LeDebat Au moins ce débat aura servi à quelque chose : confirmé que #MLP et vraiment inutile à ce pays</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>RT @alexandre_spada: Supprimer le voile à l'université : encore une preuve que @MLP_officiel n'a rien compris à la laïcité ! #2017LeDébat #…</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>RT @Rob90rtega: jurez il y en a qui voteront pour une futur présidente qui se comporte comme une gamine de lycée #2017LeDebat</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>RT @nininet37: #2017LeDebat A l'idée qu'un quart des français se laisse abuser par cette femme me donne des sueurs froides.</td>\n",
       "      <td>Nan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>#2017LeDebat @MLP_officiel invente des comptes off-shore sur son adversaire mais ne respecte pas l'indépendance de l'autorité judiciaire.</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>RT @SLeroux28: #2017LeDebat la tristesse du débat , 2 candidats finalistes qui ne sont pas à la hauteur de la fonction , du jamais vu , effara…</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>RT @SPIINGOUINOOS:   #2017LeDebat</td>\n",
       "      <td>Nan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>RT @romainribas: #2017LeDebat @EmmanuelMacron \"Mon projet pour l'école est l'école primaire où 20% des enfants ne savent pas lire écrire ou…</td>\n",
       "      <td>Nan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>RT @nininet37: #2017LeDebat A l'idée qu'un quart des français se laisse abuser par cette femme me donne des sueurs froides.</td>\n",
       "      <td>Nan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>Au moins avec @EmmanuelMacron on sait ce qu'il veut de la France et son programme on le connaît.. #2017LeDébat</td>\n",
       "      <td>Nan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>RT @nininet37: #2017LeDebat A l'idée qu'un quart des français se laisse abuser par cette femme me donne des sueurs froides.</td>\n",
       "      <td>Nan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>RT @Jean_GregP: Bon @EmmanuelMacron arrête de répondre au troll Le Pen , tu fais son jeu là. Expose ton programme et laisse la vociférer. #2…</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>RT @_WhereIsBryan_: Si après ça elle ne fait peur à personne je ne comprend pas 😂😂😂 #2017LeDebat</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>j'imagine pas comment ils transpirent xD #2017LeDébat</td>\n",
       "      <td>Nan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>RT @David_vanH: On appelle la mairie :\\n\"nous sommes sans étiquette\"\\nse défendent ils.\\nenquête , le maire est socialo.\\nMacron MafiaPS même co…</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>RT @SLeroux28: #2017LeDebat la tristesse du débat , 2 candidats finalistes qui ne sont pas à la hauteur de la fonction , du jamais vu , effara…</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>La twittosphere a ete tres bonne ce soir c'est jouissif tous ces commentaires 😂 #2017LeDebat</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>Oui #MarineLePen ne répond pas aux questions mais en attendant on sait bien que son programme est mieux que #macron  #2017LeDebat</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>RT @KIuzik: Finalement , j'ai trouvé MLP plus instable que Macron :') #2017LeDebat</td>\n",
       "      <td>Nan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>Mais c'est un débat ou ...?#2017LeDebat</td>\n",
       "      <td>Nan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>RT @TabNacim: Allez on remballe tout fin du débat\\n#2017LeDebat</td>\n",
       "      <td>Nan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>RT @junyston: Juste comme ça vous allez voter pour qui après avoir vu le débat ? #GrandDebat #2017LeDebat #RT</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>RT @GabrielVallet: Je suis pas venue ici pour souffrir ok ? #2017LeDébat</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RT @_WhereIsBryan_: Si après ça elle ne fait peur à personne je ne comprend pas 😂😂😂 #2017LeDebat</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>#2017LeDebat comment y on boucler la fin du débat en mode aller c'est bon ta fini au revoir 😂</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                   text  \\\n",
       "762                                                                      #2017LeDebat il l'a laisse pas parler c'est abusé que il coupe la parole mdrr    \n",
       "10        RT @romainribas: #2017LeDebat @EmmanuelMacron \"80% de nos médicaments sont importés\" or avec un TVA FN à 23% les prix vont augmenter (+ inf…    \n",
       "450       RT @romainribas: #2017LeDebat @EmmanuelMacron \"@MLP_officiel profite de l'échec et de la colère Elle utilise toute sa conclusion pr insulte…    \n",
       "504                                                                                 RT @Alex_Quenet: Elle a complètement pété un câble 😂😂 #2017LeDebat    \n",
       "665                                                                                                   Mon dieu la cassos 😂😂😂😂😂 #Debat2017 #2017LeDebat    \n",
       "515                                        #2017LeDebat Au moins ce débat aura servi à quelque chose : confirmé que #MLP et vraiment inutile à ce pays    \n",
       "836       RT @alexandre_spada: Supprimer le voile à l'université : encore une preuve que @MLP_officiel n'a rien compris à la laïcité ! #2017LeDébat #…    \n",
       "902                      RT @Rob90rtega: jurez il y en a qui voteront pour une futur présidente qui se comporte comme une gamine de lycée #2017LeDebat    \n",
       "401                        RT @nininet37: #2017LeDebat A l'idée qu'un quart des français se laisse abuser par cette femme me donne des sueurs froides.    \n",
       "807          #2017LeDebat @MLP_officiel invente des comptes off-shore sur son adversaire mais ne respecte pas l'indépendance de l'autorité judiciaire.    \n",
       "45     RT @SLeroux28: #2017LeDebat la tristesse du débat , 2 candidats finalistes qui ne sont pas à la hauteur de la fonction , du jamais vu , effara…    \n",
       "358                                                                                                                  RT @SPIINGOUINOOS:   #2017LeDebat    \n",
       "280       RT @romainribas: #2017LeDebat @EmmanuelMacron \"Mon projet pour l'école est l'école primaire où 20% des enfants ne savent pas lire écrire ou…    \n",
       "643                        RT @nininet37: #2017LeDebat A l'idée qu'un quart des français se laisse abuser par cette femme me donne des sueurs froides.    \n",
       "693                                     Au moins avec @EmmanuelMacron on sait ce qu'il veut de la France et son programme on le connaît.. #2017LeDébat    \n",
       "618                        RT @nininet37: #2017LeDebat A l'idée qu'un quart des français se laisse abuser par cette femme me donne des sueurs froides.    \n",
       "894      RT @Jean_GregP: Bon @EmmanuelMacron arrête de répondre au troll Le Pen , tu fais son jeu là. Expose ton programme et laisse la vociférer. #2…    \n",
       "285                                                   RT @_WhereIsBryan_: Si après ça elle ne fait peur à personne je ne comprend pas 😂😂😂 #2017LeDebat    \n",
       "711                                                                                              j'imagine pas comment ils transpirent xD #2017LeDébat    \n",
       "104  RT @David_vanH: On appelle la mairie :\\n\"nous sommes sans étiquette\"\\nse défendent ils.\\nenquête , le maire est socialo.\\nMacron MafiaPS même co…    \n",
       "48     RT @SLeroux28: #2017LeDebat la tristesse du débat , 2 candidats finalistes qui ne sont pas à la hauteur de la fonction , du jamais vu , effara…    \n",
       "499                                                       La twittosphere a ete tres bonne ce soir c'est jouissif tous ces commentaires 😂 #2017LeDebat    \n",
       "766                  Oui #MarineLePen ne répond pas aux questions mais en attendant on sait bien que son programme est mieux que #macron  #2017LeDebat    \n",
       "337                                                                 RT @KIuzik: Finalement , j'ai trouvé MLP plus instable que Macron :') #2017LeDebat    \n",
       "941                                                                                                            Mais c'est un débat ou ...?#2017LeDebat    \n",
       "174                                                                                   RT @TabNacim: Allez on remballe tout fin du débat\\n#2017LeDebat     \n",
       "93                                       RT @junyston: Juste comme ça vous allez voter pour qui après avoir vu le débat ? #GrandDebat #2017LeDebat #RT    \n",
       "806                                                                           RT @GabrielVallet: Je suis pas venue ici pour souffrir ok ? #2017LeDébat    \n",
       "32                                                    RT @_WhereIsBryan_: Si après ça elle ne fait peur à personne je ne comprend pas 😂😂😂 #2017LeDebat    \n",
       "514                                                      #2017LeDebat comment y on boucler la fin du débat en mode aller c'est bon ta fini au revoir 😂    \n",
       "\n",
       "    dic_rounded  lsvc  knn  mnb  lsvc_bin  knn_bin  mnb_bin  \n",
       "762         Nan     2    2    0         0        0        0  \n",
       "10            1     2    2    2         0        0        1  \n",
       "450           0     2    2    2         0        0        0  \n",
       "504           0     2    2    2         0        1        0  \n",
       "665         Nan     2    2    2         0        0        0  \n",
       "515           0     2    2    2         1        1        1  \n",
       "836           0     2    2    2         0        1        0  \n",
       "902           1     2    2    2         1        1        0  \n",
       "401         Nan     2    2    2         0        0        0  \n",
       "807           0     2    2    2         0        0        1  \n",
       "45            0     2    2    2         0        1        0  \n",
       "358         Nan     2    2    2         0        0        1  \n",
       "280         Nan     2    2    2         0        0        1  \n",
       "643         Nan     2    2    2         0        0        0  \n",
       "693         Nan     2    2    2         1        0        0  \n",
       "618         Nan     2    2    2         0        0        0  \n",
       "894           1     2    2    2         0        1        0  \n",
       "285           0     2    2    2         0        0        0  \n",
       "711         Nan     2    2    2         0        1        0  \n",
       "104           1     2    2    2         0        0        0  \n",
       "48            0     2    2    2         0        1        0  \n",
       "499           1     2    2    2         1        1        0  \n",
       "766           1     1    2    1         1        1        1  \n",
       "337         Nan     2    2    1         0        1        0  \n",
       "941         Nan     2    2    2         0        0        0  \n",
       "174         Nan     2    2    2         1        1        1  \n",
       "93            1     2    2    2         1        1        1  \n",
       "806           0     2    2    2         0        0        1  \n",
       "32            0     2    2    2         0        0        0  \n",
       "514           1     2    2    2         0        0        0  "
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[['text', 'dic_rounded','lsvc', 'knn', 'mnb', 'lsvc_bin', 'knn_bin', 'mnb_bin']].sample(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos los tweets con su polaridad y coordenadas para situarlos en el mapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets[['text','dic_rounded','lsvc', 'knn', 'mnb', 'lsvc_bin', 'knn_bin', 'mnb_bin']].to_csv('tweets_debat_completos/Centre-Val de Loire.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
